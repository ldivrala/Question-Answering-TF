{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "TFQATraining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ldivrala/Question-Answering-TF/blob/main/TFQATraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGYNcn6DApg-"
      },
      "source": [
        "Install required library"
      ],
      "id": "KGYNcn6DApg-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW-2h4Y16cav"
      },
      "source": [
        "# !pip install tensorflow_text\n",
        "# !pip install sentencepiece\n"
      ],
      "id": "nW-2h4Y16cav",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed0ce9b9"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import tensorflow.experimental.numpy as tnp\n",
        "import tensorflow_hub as hub\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "import tensorflow_text\n",
        "import sentencepiece as spm"
      ],
      "id": "ed0ce9b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQs4yucWA0iY"
      },
      "source": [
        "Connect colab with notebook"
      ],
      "id": "WQs4yucWA0iY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy089mO2GLHS",
        "outputId": "fd9ea3a5-aeb4-43e2-9760-12801ecf86b8"
      },
      "source": [
        "# Connect with Google Drive for dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Iy089mO2GLHS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0fCxcErz6Sk"
      },
      "source": [
        ""
      ],
      "id": "m0fCxcErz6Sk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "266b99a8",
        "outputId": "da651f6b-f5d1-4db7-e864-a69e498f017d"
      },
      "source": [
        "root_dir = \"/content/drive/MyDrive/Dataset/datasets/\"\n",
        "\n",
        "trainset_df_orig = pd.read_json(root_dir + \"train-v2.0.json\", encoding='utf-8')\n",
        "devset_df_orig = pd.read_json(root_dir +\"dev-v2.0.json\", encoding='utf-8')\n",
        "\n",
        "trainset_df_orig.head()"
      ],
      "id": "266b99a8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>version</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Beyoncé', 'paragraphs': [{'qas': [{...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Frédéric_Chopin', 'paragraphs': [{'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Sino-Tibetan_relations_during_the_M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'IPod', 'paragraphs': [{'qas': [{'qu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'The_Legend_of_Zelda:_Twilight_Princ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  version                                               data\n",
              "0    v2.0  {'title': 'Beyoncé', 'paragraphs': [{'qas': [{...\n",
              "1    v2.0  {'title': 'Frédéric_Chopin', 'paragraphs': [{'...\n",
              "2    v2.0  {'title': 'Sino-Tibetan_relations_during_the_M...\n",
              "3    v2.0  {'title': 'IPod', 'paragraphs': [{'qas': [{'qu...\n",
              "4    v2.0  {'title': 'The_Legend_of_Zelda:_Twilight_Princ..."
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt0C1skWHwbI",
        "outputId": "a3ff5d93-f6ed-4ac3-a78b-d8701b6827dc"
      },
      "source": [
        "trainset_df_orig[\"data\"][0][\"paragraphs\"][:1]"
      ],
      "id": "Tt0C1skWHwbI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
              "  'qas': [{'answers': [{'answer_start': 269, 'text': 'in the late 1990s'}],\n",
              "    'id': '56be85543aeaaa14008c9063',\n",
              "    'is_impossible': False,\n",
              "    'question': 'When did Beyonce start becoming popular?'},\n",
              "   {'answers': [{'answer_start': 207, 'text': 'singing and dancing'}],\n",
              "    'id': '56be85543aeaaa14008c9065',\n",
              "    'is_impossible': False,\n",
              "    'question': 'What areas did Beyonce compete in when she was growing up?'},\n",
              "   {'answers': [{'answer_start': 526, 'text': '2003'}],\n",
              "    'id': '56be85543aeaaa14008c9066',\n",
              "    'is_impossible': False,\n",
              "    'question': \"When did Beyonce leave Destiny's Child and become a solo singer?\"},\n",
              "   {'answers': [{'answer_start': 166, 'text': 'Houston, Texas'}],\n",
              "    'id': '56bf6b0f3aeaaa14008c9601',\n",
              "    'is_impossible': False,\n",
              "    'question': 'In what city and state did Beyonce  grow up? '},\n",
              "   {'answers': [{'answer_start': 276, 'text': 'late 1990s'}],\n",
              "    'id': '56bf6b0f3aeaaa14008c9602',\n",
              "    'is_impossible': False,\n",
              "    'question': 'In which decade did Beyonce become famous?'},\n",
              "   {'answers': [{'answer_start': 320, 'text': \"Destiny's Child\"}],\n",
              "    'id': '56bf6b0f3aeaaa14008c9603',\n",
              "    'is_impossible': False,\n",
              "    'question': 'In what R&B group was she the lead singer?'},\n",
              "   {'answers': [{'answer_start': 505, 'text': 'Dangerously in Love'}],\n",
              "    'id': '56bf6b0f3aeaaa14008c9604',\n",
              "    'is_impossible': False,\n",
              "    'question': 'What album made her a worldwide known artist?'},\n",
              "   {'answers': [{'answer_start': 360, 'text': 'Mathew Knowles'}],\n",
              "    'id': '56bf6b0f3aeaaa14008c9605',\n",
              "    'is_impossible': False,\n",
              "    'question': \"Who managed the Destiny's Child group?\"},\n",
              "   {'answers': [{'answer_start': 276, 'text': 'late 1990s'}],\n",
              "    'id': '56d43c5f2ccc5a1400d830a9',\n",
              "    'is_impossible': False,\n",
              "    'question': 'When did Beyoncé rise to fame?'},\n",
              "   {'answers': [{'answer_start': 290, 'text': 'lead singer'}],\n",
              "    'id': '56d43c5f2ccc5a1400d830aa',\n",
              "    'is_impossible': False,\n",
              "    'question': \"What role did Beyoncé have in Destiny's Child?\"},\n",
              "   {'answers': [{'answer_start': 505, 'text': 'Dangerously in Love'}],\n",
              "    'id': '56d43c5f2ccc5a1400d830ab',\n",
              "    'is_impossible': False,\n",
              "    'question': 'What was the first album Beyoncé released as a solo artist?'},\n",
              "   {'answers': [{'answer_start': 526, 'text': '2003'}],\n",
              "    'id': '56d43c5f2ccc5a1400d830ac',\n",
              "    'is_impossible': False,\n",
              "    'question': 'When did Beyoncé release Dangerously in Love?'},\n",
              "   {'answers': [{'answer_start': 590, 'text': 'five'}],\n",
              "    'id': '56d43c5f2ccc5a1400d830ad',\n",
              "    'is_impossible': False,\n",
              "    'question': 'How many Grammy awards did Beyoncé win for her first solo album?'},\n",
              "   {'answers': [{'answer_start': 290, 'text': 'lead singer'}],\n",
              "    'id': '56d43ce42ccc5a1400d830b4',\n",
              "    'is_impossible': False,\n",
              "    'question': \"What was Beyoncé's role in Destiny's Child?\"},\n",
              "   {'answers': [{'answer_start': 505, 'text': 'Dangerously in Love'}],\n",
              "    'id': '56d43ce42ccc5a1400d830b5',\n",
              "    'is_impossible': False,\n",
              "    'question': \"What was the name of Beyoncé's first solo album?\"}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux4kuYS4A_JB"
      },
      "source": [
        "Unzip albert module from drive"
      ],
      "id": "ux4kuYS4A_JB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ghWeXK00-Cl"
      },
      "source": [
        "!mkdir /content/sample_data/albert_base_3\n",
        "!mkdir /content/sample_data/albert_en_preprocess_3\n",
        "\n",
        "!tar xf /content/drive/MyDrive/Models/Albert/albert_base_3.tar.gz -C /content/sample_data/albert_base_3\n",
        "!tar xf /content/drive/MyDrive/Models/Albert/albert_en_preprocess_3.tar.gz -C /content/sample_data/albert_en_preprocess_3"
      ],
      "id": "_ghWeXK00-Cl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w4EhC8UBG8h"
      },
      "source": [
        "Tokenizer"
      ],
      "id": "_w4EhC8UBG8h"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8_swc4y1At9"
      },
      "source": [
        "tokenizer = spm.SentencePieceProcessor(\"/content/sample_data/albert_base_3/assets/30k-clean.model\")"
      ],
      "id": "h8_swc4y1At9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "zNdF7oACBLzv",
        "outputId": "658fce03-4b17-4a60-dd52-bc7fcf651c9b"
      },
      "source": [
        "Get our dataset"
      ],
      "id": "zNdF7oACBLzv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-c06fca90f7e0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Get our dataset\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84b997d4"
      },
      "source": [
        "def get_dataset(data_array):\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    \n",
        "    for data in data_array:\n",
        "        for paragraph in data[\"paragraphs\"]:\n",
        "\n",
        "            context = paragraph[\"context\"]\n",
        "            for qas in paragraph[\"qas\"]:\n",
        "                que = qas[\"question\"]\n",
        "\n",
        "                if \"plausible_answers\" in qas:\n",
        "                    qas[\"answers\"] = qas[\"plausible_answers\"]\n",
        "\n",
        "                if len(qas[\"answers\"]) > 0:\n",
        "                  answer_start = qas[\"answers\"][0][\"answer_start\"]\n",
        "                  answer_text = qas[\"answers\"][0][\"text\"]\n",
        "\n",
        "                  que_tokenize_length = len(tokenizer.tokenize(que.strip().lower()))\n",
        "                  context_before_length = len(tokenizer.tokenize(context[:answer_start].strip().lower()))\n",
        "                  answer_tokenize_length = len(tokenizer.tokenize(answer_text.strip().lower()))\n",
        "\n",
        "                  ans = [que_tokenize_length + 2 + context_before_length, \n",
        "                          que_tokenize_length + 2 + context_before_length + answer_tokenize_length]\n",
        "\n",
        "                  # contexts[:answer_start]\n",
        "                else:\n",
        "                  ans = \"<NO_ANSWER>\"\n",
        "\n",
        "                contexts.append(context)\n",
        "                questions.append(que)\n",
        "                answers.append(ans)\n",
        "                \n",
        "            \n",
        "    dataset = pd.DataFrame({\"que\": questions, \"ans\": answers, \"context\": contexts})\n",
        "    return dataset"
      ],
      "id": "84b997d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "184f4122",
        "outputId": "5661638b-3778-4ba1-b83e-bb24235edf8e"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "trainset_orig = get_dataset(trainset_df_orig[\"data\"])\n",
        "trainset_orig = shuffle(trainset_orig)\n",
        "\n",
        "trainset_orig.reset_index(inplace = True,  drop=True)\n",
        "\n",
        "# devset_orig = get_dataset(devset_df_orig[\"data\"])\n",
        "trainset_orig.head()"
      ],
      "id": "184f4122",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>que</th>\n",
              "      <th>ans</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is Melbourne's highest temperature recorded?</td>\n",
              "      <td>[180, 195]</td>\n",
              "      <td>Melbourne is also prone to isolated convective...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What wasn't the Soviet response to the Nazi-So...</td>\n",
              "      <td>[49, 55]</td>\n",
              "      <td>In response to the publication of the secret p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What country has the longest Constitution?</td>\n",
              "      <td>[10, 11]</td>\n",
              "      <td>In India, the longest constitutional text in t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How long of a running time did the Number 199 ...</td>\n",
              "      <td>[180, 185]</td>\n",
              "      <td>Britain had successful tested a new HAA gun, 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The period between 640-580 BC was known as what?</td>\n",
              "      <td>[25, 28]</td>\n",
              "      <td>In the first large-scale depictions during the...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 que  ...                                            context\n",
              "0  What is Melbourne's highest temperature recorded?  ...  Melbourne is also prone to isolated convective...\n",
              "1  What wasn't the Soviet response to the Nazi-So...  ...  In response to the publication of the secret p...\n",
              "2         What country has the longest Constitution?  ...  In India, the longest constitutional text in t...\n",
              "3  How long of a running time did the Number 199 ...  ...  Britain had successful tested a new HAA gun, 3...\n",
              "4   The period between 640-580 BC was known as what?  ...  In the first large-scale depictions during the...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJXvknY-VXhT",
        "outputId": "e20f9ac2-897c-422a-9cd5-8b387ebe2d7e"
      },
      "source": [
        "tokenizer.EncodeAsPieces(\"hii\")"
      ],
      "id": "TJXvknY-VXhT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁hi', 'i']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tv2PEeWKsJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c3e596-3c50-48e0-a448-f4a9044afd0e"
      },
      "source": [
        "len(trainset_orig)"
      ],
      "id": "5tv2PEeWKsJg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "130319"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O2-pKXkBUZV"
      },
      "source": [
        "Preprocessor for sentences"
      ],
      "id": "6O2-pKXkBUZV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3lBHsXeSc4X"
      },
      "source": [
        "preprocessor = hub.load(\"http://tfhub.dev/tensorflow/albert_en_preprocess/3\")\n",
        "\n",
        "# Step 1: tokenize batches of text inputs.\n",
        "text_inputs = [tf.keras.layers.Input(shape=(), dtype=tf.string), tf.keras.layers.Input(shape=(), dtype=tf.string)] # This SavedModel accepts up to 2 text inputs.\n",
        "tokenize = hub.KerasLayer(preprocessor.tokenize)\n",
        "tokenized_inputs = [tokenize(segment) for segment in text_inputs]\n",
        "\n",
        "# Step 2 (optional): modify tokenized inputs.\n",
        "pass\n",
        "\n",
        "# Step 3: pack input sequences for the Transformer encoder.\n",
        "seq_length = 512  # Your choice here.\n",
        "bert_pack_inputs = hub.KerasLayer(\n",
        "    preprocessor.bert_pack_inputs,\n",
        "    arguments=dict(seq_length=seq_length))  # Optional argument.\n",
        "\n",
        "bert_pack_output = bert_pack_inputs(tokenized_inputs)\n",
        "encoder_inputs = keras.Model(text_inputs, bert_pack_output)"
      ],
      "id": "d3lBHsXeSc4X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weM406_4PYg6",
        "outputId": "faadfd70-8ce8-4bd8-efc0-04e2ca360c19"
      },
      "source": [
        "encoder_inputs([tf.constant([\"hii\", \"byy\"]), tf.constant([\"hii\", \"chai\"])])"
      ],
      "id": "weM406_4PYg6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_mask': <tf.Tensor: shape=(2, 512), dtype=int32, numpy=\n",
              " array([[1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>,\n",
              " 'input_type_ids': <tf.Tensor: shape=(2, 512), dtype=int32, numpy=\n",
              " array([[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>,\n",
              " 'input_word_ids': <tf.Tensor: shape=(2, 512), dtype=int32, numpy=\n",
              " array([[   2, 4148,   49, ...,    0,    0,    0],\n",
              "        [   2,   34,   93, ...,    0,    0,    0]], dtype=int32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvknosAaBbjo"
      },
      "source": [
        "Dataset pipeline creation"
      ],
      "id": "SvknosAaBbjo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e247301d"
      },
      "source": [
        "batch_size = 4\n",
        "def format_dataset():\n",
        "    \n",
        "    for i in range(len(trainset_orig)):\n",
        "        que = trainset_orig.loc[i, \"que\"].strip().lower()\n",
        "        context = trainset_orig.loc[i, \"context\"].strip().lower()\n",
        "        answer = trainset_orig.loc[i, \"ans\"]\n",
        "        if answer[0] >= seq_length:\n",
        "          answer[0] = 510\n",
        "        \n",
        "        if answer[1] >= seq_length:\n",
        "          answer[1] = 511\n",
        "\n",
        "\n",
        "        yield ({\n",
        "            \"question_inputs\": que,\n",
        "            \"context_inputs\": context,\n",
        "        }, {\n",
        "            \"answer_start_outputs\": answer[0],\n",
        "            \"answer_end_outputs\": answer[1]\n",
        "        })\n",
        "    \n",
        "\n",
        "def make_dataset():\n",
        "    dataset = tf.data.Dataset.from_generator(format_dataset, \n",
        "                                             output_signature =({\n",
        "                                                 \"question_inputs\": tf.TensorSpec(shape=(), dtype=tf.string, name=\"question_inputs\"),\n",
        "                                                 \"context_inputs\": tf.TensorSpec(shape=(), dtype=tf.string, name=\"context_inputs\"),\n",
        "                                             },\n",
        "                                              {\n",
        "                                                \"answer_start_outputs\" : tf.TensorSpec(shape=(), dtype=tf.int64, name=\"answer_start_outputs\"),\n",
        "                                                \"answer_end_outputs\" : tf.TensorSpec(shape=(), dtype=tf.int64, name=\"answer_end_outputs\")\n",
        "                                              }\n",
        "                                            ))\n",
        "    dataset = dataset.cache().batch(batch_size)\n",
        "    \n",
        "    \n",
        "    return dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "id": "e247301d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25e28b1a"
      },
      "source": [
        ""
      ],
      "id": "25e28b1a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42da1d0a"
      },
      "source": [
        "train_ds = make_dataset()"
      ],
      "id": "42da1d0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a40f0f93",
        "outputId": "9a38bbac-92aa-4d65-c245-6a38161b25f1"
      },
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"question_inputs\", \"context_inputs\"].shape: {inputs[\"question_inputs\"].shape, inputs[\"context_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets['answer_start_outputs'].shape}\")\n",
        "\n",
        "    context = inputs[\"context_inputs\"][1].numpy().decode()\n",
        "    question = inputs[\"question_inputs\"][1].numpy().decode()\n",
        "    print(question)\n",
        "    print(context)\n",
        "    answer = [\"[CLS]\"] + tokenizer.EncodeAsPieces(question) + [\"[SEP]\"] + tokenizer.EncodeAsPieces(context) +  [\"[SEP]\"]\n",
        "    \n",
        "    answer = answer[int(targets[\"answer_start_outputs\"][1].numpy()):int(targets[\"answer_end_outputs\"][1].numpy())]\n",
        "    print(answer)"
      ],
      "id": "a40f0f93",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"question_inputs\", \"context_inputs\"].shape: (TensorShape([4]), TensorShape([4]))\n",
            "targets.shape: (4,)\n",
            "what wasn't the soviet response to the nazi-soviet relations publication\n",
            "in response to the publication of the secret protocols and other secret german–soviet relations documents in the state department edition nazi–soviet relations (1948), stalin published falsifiers of history, which included the claim that, during the pact's operation, stalin rejected hitler's claim to share in a division of the world, without mentioning the soviet offer to join the axis. that version persisted, without exception, in historical studies, official accounts, memoirs and textbooks published in the soviet union until the soviet union's dissolution.\n",
            "['▁fal', 's', 'ifier', 's', '▁of', '▁history']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5lKbJjoBhEU"
      },
      "source": [
        "Create model layers"
      ],
      "id": "d5lKbJjoBhEU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94ebcd65"
      },
      "source": [
        "class TextContextualEmbedding(keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "      super(TextContextualEmbedding, self).__init__(**kwargs)\n",
        "      encoder = hub.KerasLayer(\n",
        "                  \"https://tfhub.dev/tensorflow/albert_en_base/3\",\n",
        "                  trainable=True)\n",
        "      self.albert_module = encoder\n",
        "      \n",
        "  def call(self, encoder_inputs):\n",
        "\n",
        "      albert_inputs = dict(\n",
        "          input_ids=encoder_inputs[\"input_word_ids\"],\n",
        "          input_mask= encoder_inputs[\"input_mask\"],\n",
        "          segment_ids=encoder_inputs[\"input_type_ids\"])\n",
        "      \n",
        "      albert_outputs = self.albert_module(encoder_inputs)\n",
        "      return albert_outputs[\"sequence_output\"]\n",
        "\n",
        "  def compute_mask(self, inputs, mask = None):\n",
        "      return inputs[\"input_mask\"]\n",
        "\n",
        "class TransformerEncoder(keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [keras.layers.Dense(dense_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = keras.layers.LayerNormalization()\n",
        "        self.layernorm_2 = keras.layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "           \n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        \n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "  \n",
        "    def get_config(self):\n",
        "        config = super(TransformerEncoder, self).get_config()\n",
        "\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "        })\n",
        "        return config"
      ],
      "id": "94ebcd65",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUv0ZMBJBlRN"
      },
      "source": [
        "Model building"
      ],
      "id": "wUv0ZMBJBlRN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21800a8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb2f72ba-a8a0-4d03-a6b9-8b7c05d974da"
      },
      "source": [
        "embed_dim = 768\n",
        "num_heads = 8\n",
        "latent_dim = 768\n",
        "vocab_size = preprocessor.tokenize.get_special_tokens_dict()[\"vocab_size\"]\n",
        "sequence_length = seq_length\n",
        "\n",
        "question_inputs = keras.Input(shape=(), dtype=tf.string, name=\"question_inputs\")\n",
        "context_inputs = keras.Input(shape=(), dtype=tf.string, name=\"context_inputs\")\n",
        "\n",
        "input_token = encoder_inputs([question_inputs, context_inputs])\n",
        "\n",
        "# x = bert_pack_inputs([input_token])\n",
        "encoder_outputs = TextContextualEmbedding()(input_token)\n",
        "\n",
        "x = TransformerEncoder(embed_dim, latent_dim, num_heads)(encoder_outputs)\n",
        "\n",
        "x = keras.layers.Dropout(0.4)(x)\n",
        "x1 = keras.layers.Dense(1)(x)\n",
        "x2 = keras.layers.Dense(1)(x)\n",
        "\n",
        "x1 = keras.layers.Reshape((-1,))(x1)\n",
        "x2 = keras.layers.Reshape((-1,))(x2)\n",
        "\n",
        "output1 = keras.layers.Activation(\"softmax\", name =\"answer_start_outputs\")(x1)\n",
        "output2 = keras.layers.Activation(\"softmax\", name =\"answer_end_outputs\")(x2)\n",
        "\n",
        "transformer = keras.Model([question_inputs, context_inputs], [output1, output2])\n",
        "transformer.summary()"
      ],
      "id": "21800a8a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "question_inputs (InputLayer)    [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "context_inputs (InputLayer)     [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model (Functional)              {'input_type_ids': ( 0           question_inputs[0][0]            \n",
            "                                                                 context_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "text_contextual_embedding (Text (None, 512, 768)     11683584    model[0][0]                      \n",
            "                                                                 model[0][1]                      \n",
            "                                                                 model[0][2]                      \n",
            "__________________________________________________________________________________________________\n",
            "transformer_encoder (Transforme (None, 512, 768)     20077824    text_contextual_embedding[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 512, 768)     0           transformer_encoder[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 512, 1)       769         dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 512, 1)       769         dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 512)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 512)          0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "answer_start_outputs (Activatio (None, 512)          0           reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "answer_end_outputs (Activation) (None, 512)          0           reshape_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 31,762,946\n",
            "Trainable params: 31,762,946\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVQDp901Bnoq"
      },
      "source": [
        "Train our model"
      ],
      "id": "sVQDp901Bnoq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t42oD8ijPyU4"
      },
      "source": [
        "transformer.layers[3].trainable = False"
      ],
      "id": "t42oD8ijPyU4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct3kQH6U7Nu7",
        "outputId": "bb35884d-36b5-4ae4-90a2-6ee71906dc1a"
      },
      "source": [
        "epochs = 1  # This should be at least 50 for convergence\n",
        "\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"]\n",
        ")\n",
        "\n",
        "transformer.fit(train_ds, epochs=epochs, use_multiprocessing=True)\n",
        "\n",
        "# Save the entire model as a SavedModel.\n",
        "transformer.save('saved_model/my_model')\n",
        "transformer.save_weights('saved_model/weights')"
      ],
      "id": "Ct3kQH6U7Nu7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32580/32580 [==============================] - 8625s 264ms/step - loss: 9.8643 - answer_start_outputs_loss: 4.8919 - answer_end_outputs_loss: 4.9724 - answer_start_outputs_acc: 0.0361 - answer_end_outputs_acc: 0.0217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as keras_layer_2_layer_call_and_return_conditional_losses, keras_layer_2_layer_call_fn, multi_head_attention_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses while saving (showing 5 of 140). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n",
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYgkN3cWpixL",
        "outputId": "ba5e99dc-9651-4f56-b21d-edc8f100749c"
      },
      "source": [
        "transformer.layers[3].trainable = True\n",
        "epochs = 1  # This should be at least 500 for convergence\n",
        "\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"]\n",
        ")\n",
        "\n",
        "transformer.fit(train_ds, epochs=epochs, use_multiprocessing=True)\n",
        "\n",
        "# Save the entire model as a SavedModel.\n",
        "transformer.save('saved_model/my_model')\n",
        "transformer.save_weights('saved_model/weights')"
      ],
      "id": "qYgkN3cWpixL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  16660/Unknown - 11379s 682ms/step - loss: 12.5040 - answer_start_outputs_loss: 6.2524 - answer_end_outputs_loss: 6.2517 - answer_start_outputs_acc: 0.0019 - answer_end_outputs_acc: 0.0022"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNAu-K-yzdCg"
      },
      "source": [
        ""
      ],
      "id": "zNAu-K-yzdCg",
      "execution_count": null,
      "outputs": []
    }
  ]
}